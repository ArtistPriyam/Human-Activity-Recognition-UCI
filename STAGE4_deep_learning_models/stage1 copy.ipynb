{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fde8e83",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7430c013",
   "metadata": {},
   "source": [
    "## FEW CONCEPTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b16bf4",
   "metadata": {},
   "source": [
    "### what mean by timestamps and inertial signals ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5072dfd2",
   "metadata": {},
   "source": [
    "\n",
    "Inertial signals are measurements from sensors like accelerometers and gyroscopes that capture motion—specifically linear acceleration and angular velocity. In the UCI HAR dataset, these include signals such as body acceleration, gyroscope data, and total acceleration along the X, Y, and Z axes. That gives a total of 9 signals.\n",
    "\n",
    "Time stamps refer to the sequence of measurements taken over time. The dataset is divided into windows of 2.56 seconds, sampled at 50 Hz, meaning each window contains 128 time steps. So, for each sample, you have 128 values for each of the 9 signals, forming a matrix of shape (128, 9) per sample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5b68fb",
   "metadata": {},
   "source": [
    "### reason for shape ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a514e1c7",
   "metadata": {},
   "source": [
    "\n",
    "the shape `(10299, 128, 9)` is **correct** for the UCI HAR Dataset **Inertial Signals** when processed properly. Here's what each dimension means:\n",
    "\n",
    "* `10299`: Total number of samples (train + test combined: 7352 train + 2947 test)\n",
    "* `128`: Each sample is a time window of 128 readings (time steps)\n",
    "* `9`: There are **9 inertial signal features**:\n",
    "  * Total Acc: `body_acc_x/y/z`\n",
    "  * Body Acc: `body_gyro_x/y/z`\n",
    "  * Jerk: `total_acc_x/y/z`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f31337",
   "metadata": {},
   "source": [
    "### 📊 Why the **train_data has 561 features**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448bdc59",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- The 561 features come from **hand-crafted feature extraction** on the raw signal data.\n",
    "⚙️ Step-by-step:\n",
    "\n",
    "1. **Raw data shape** (after stacking `Inertial Signals`):\n",
    "\n",
    "   * Shape = `(7352, 128, 9)` for training\n",
    "   * Raw signal from accelerometer + gyroscope in time windows.\n",
    "\n",
    "2. **Feature extraction process** (already done in the dataset's `X_train.txt` file):\n",
    "\n",
    "   * From each 128-sample × 9-signal window → extract statistical features:\n",
    "\n",
    "     * e.g., mean, std, energy, entropy, correlation, FFT coefficients, etc.\n",
    "     * This is done per signal.\n",
    "   * In total, **561 features per sample** are engineered this way.\n",
    "\n",
    "3. **Final train dataset used in classification:**\n",
    "\n",
    "   * Shape = `(7352, 561)`\n",
    "   * Each row is a sample (1 time window)\n",
    "   * Each column is a feature\n",
    " 📁 Files Involved:\n",
    "\n",
    "* `Inertial Signals/` → Raw 128×9 time series\n",
    "* `X_train.txt` → Preprocessed data with 561 features\n",
    "* `y_train.txt` → Corresponding activity labels\n",
    "\n",
    "If we're working from raw Inertial Signals, you'll have to **re-implement feature extraction** to get the 561 features — or use the `X_train.txt` directly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cb54f4",
   "metadata": {},
   "source": [
    "# concatatanating the signal files to create raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0046410d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddd2ee84",
   "metadata": {},
   "source": [
    "#### data form ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace3f6ca",
   "metadata": {},
   "source": [
    "\n",
    "⚠️ Only Consider Feature Selection If:\n",
    "- You're doing benchmarking with traditional ML models.\n",
    "- You want to reduce model complexity for ultra-low power devices (like wearable deployment).\n",
    "- You're doing multisensor fusion and need to discard irrelevant channels.\n",
    "- ✅ Option 1: Use Provided X_train.txt and X_test.txt\n",
    "- Ready to use, already feature-extracted.\n",
    "- Shape: (7352, 561) for train.\n",
    "- Good if you want to compare with traditional ML or get quick results with dense/deep networks.\n",
    "> ✅ Option 2: Use Raw Inertial Signals/ and Build Custom Data\n",
    "- You get full time series signals:\n",
    "- body_acc_x_train.txt, body_gyro_y_test.txt, etc.\n",
    "- Shape: (7352, 128) per signal\n",
    "- Combine the 9 signals to form shape: (7352, 128, 9)\n",
    "- Ideal for deep learning: LSTM, CNN-LSTM, GRU, Transformers.\n",
    "> 🔥 Recommended for Deep Learning:\n",
    "- 💡 Use raw signals from Inertial Signals/ to build a 3D array:\n",
    "- (samples, timesteps=128, features=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfebe531",
   "metadata": {},
   "source": [
    "#### loading the files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ead2a1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_3264/2521903796.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_3264/2521903796.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_3264/2521903796.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_3264/2521903796.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_3264/2521903796.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_3264/2521903796.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_3264/2521903796.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_3264/2521903796.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_3264/2521903796.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_3264/2521903796.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_3264/2521903796.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_3264/2521903796.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_3264/2521903796.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_3264/2521903796.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_3264/2521903796.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_3264/2521903796.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_3264/2521903796.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_3264/2521903796.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_signals(signal_dir):\n",
    "    filenames = sorted(os.listdir(signal_dir))\n",
    "    signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None) \n",
    "                   for f in filenames]\n",
    "    return np.stack(signal_data, axis=-1)  # shape: (samples, time, features)\n",
    "\n",
    "x_train_raw = load_signals(\"/Users/priyam/paper_recreation/UCI_HAR_FEATURE_ANALYSIS_MODELLING/UCI HAR Dataset/train/Inertial Signals 1\")\n",
    "\n",
    "x_test_raw = load_signals(\"/Users/priyam/paper_recreation/UCI_HAR_FEATURE_ANALYSIS_MODELLING/UCI HAR Dataset/test/Inertial Signals\")\n",
    "# Combine train and test data\n",
    "x_raw = np.concatenate((x_train_raw, x_test_raw), axis=0)\n",
    "y_train = pd.read_csv(\"/Users/priyam/paper_recreation/UCI_HAR_FEATURE_ANALYSIS_MODELLING/UCI HAR Dataset/train/y_train.CSV\", header=None).values.flatten()\n",
    "y_test = pd.read_csv(\"/Users/priyam/paper_recreation/UCI_HAR_FEATURE_ANALYSIS_MODELLING/UCI HAR Dataset/test/y_test.CSV\", header=None).values.flatten()\n",
    "# Combine train and test labels\n",
    "y = np.concatenate((y_train, y_test), axis=0)\n",
    "# convert x_raw to a DataFrame without flattening\n",
    "#x = pd.DataFrame(x_raw)  # shape: (samples, time * features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef94d030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10299, 128, 9)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_raw.shape  # Check the shape of the raw data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ed5ca76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10299,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a6455",
   "metadata": {},
   "source": [
    "## CONVERTING INTO TENSOR "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ae9df7",
   "metadata": {},
   "source": [
    "### NORMALIZE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a27ab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def normalize_signals(x_raw):\n",
    "    num_samples, time_steps, num_features = x_raw.shape\n",
    "    x_reshaped = x_raw.reshape(-1, num_features)  # (samples * time_steps, features)\n",
    "    scaler = StandardScaler()\n",
    "    x_scaled = scaler.fit_transform(x_reshaped).reshape(num_samples, time_steps, num_features)\n",
    "    return x_scaled\n",
    "\n",
    "x = normalize_signals(x_raw)  # shape: (samples, 128, 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50170bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "102296c8",
   "metadata": {},
   "source": [
    "### ONE HOT ENCODING OF LABELS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5db2c62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Convert y from 1-6 to 0-5 (zero-indexed)\n",
    "y = y - 1\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = encoder.fit_transform(y.reshape(-1, 1))  # shape: (samples, 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1701aed",
   "metadata": {},
   "source": [
    "##### why we use one hot encoding ?\n",
    "   - ❓ Why One-Hot Encoding?\n",
    "   - You're one-hot encoding the y labels because:\n",
    "    \n",
    "   - The labels are categorical (e.g., 0–5 for 6 activities).\n",
    "   - Many ML models (especially neural networks) don't handle categorical variables directly.\n",
    "   - One-hot encoding converts labels like 2 into [0, 0, 1, 0, 0, 0], making it easier for models to understand.\n",
    "   - This is especially important for classification tasks using neural networks.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf934684",
   "metadata": {},
   "source": [
    "### TRAIN TEST SPLIT FINAL CONVERSION INTO TENSOR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c228a6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y_encoded, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f576b99",
   "metadata": {},
   "source": [
    "# FEATURE EXTRACTION FROM RAW_FILE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d8b745f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "from scipy.fft import fft\n",
    "\n",
    "def extract_features_from_window(window):\n",
    "    features = []\n",
    "    for i, signal in enumerate(window.T):  # window.T shape (9, 128) → iterate over 9 signals\n",
    "        # Time-domain features\n",
    "        features.append(np.mean(signal))\n",
    "        features.append(np.std(signal))\n",
    "        features.append(np.min(signal))\n",
    "        features.append(np.max(signal))\n",
    "        features.append(np.median(signal))\n",
    "        features.append(skew(signal))\n",
    "        features.append(kurtosis(signal))\n",
    "\n",
    "        # Frequency-domain features\n",
    "        fft_vals = np.abs(fft(signal))\n",
    "        fft_norm = fft_vals / (np.sum(fft_vals) + 1e-12)  # avoid div zero\n",
    "        features.append(np.sum(fft_vals**2))              # Energy\n",
    "        features.append(entropy(fft_norm + 1e-12))        # Entropy\n",
    "        features.append(np.mean(fft_vals))                 # Mean power\n",
    "        features.append(np.argmax(fft_vals))               # Max freq index (dominant frequency)\n",
    "    return features\n",
    "\n",
    "def extract_features_from_all_windows(x_raw):\n",
    "    feature_names = []\n",
    "    signals = ['body_acc_x', 'body_acc_y', 'body_acc_z',\n",
    "               'body_gyro_x', 'body_gyro_y', 'body_gyro_z',\n",
    "               'total_acc_x', 'total_acc_y', 'total_acc_z']\n",
    "    stats = ['mean', 'std', 'min', 'max', 'median', 'skew', 'kurtosis',\n",
    "             'energy', 'entropy', 'mean_power', 'max_freq_idx']\n",
    "    \n",
    "    for sig in signals:\n",
    "        for stat in stats:\n",
    "            feature_names.append(f\"{sig}_{stat}\")\n",
    "    \n",
    "    all_features = []\n",
    "    for window in x_raw:\n",
    "        feats = extract_features_from_window(window)\n",
    "        all_features.append(feats)\n",
    "        \n",
    "    df_features = pd.DataFrame(all_features, columns=feature_names)\n",
    "    return df_features\n",
    "\n",
    "# Usage:\n",
    "# x_features_df = extract_features_from_all_windows(x_raw)\n",
    "# print(x_features_df.shape)  # (samples, 9*11=99 features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75f439fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = extract_features_from_all_windows(x_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "092849cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_acc_x_mean</th>\n",
       "      <th>body_acc_x_std</th>\n",
       "      <th>body_acc_x_min</th>\n",
       "      <th>body_acc_x_max</th>\n",
       "      <th>body_acc_x_median</th>\n",
       "      <th>body_acc_x_skew</th>\n",
       "      <th>body_acc_x_kurtosis</th>\n",
       "      <th>body_acc_x_energy</th>\n",
       "      <th>body_acc_x_entropy</th>\n",
       "      <th>body_acc_x_mean_power</th>\n",
       "      <th>...</th>\n",
       "      <th>total_acc_z_std</th>\n",
       "      <th>total_acc_z_min</th>\n",
       "      <th>total_acc_z_max</th>\n",
       "      <th>total_acc_z_median</th>\n",
       "      <th>total_acc_z_skew</th>\n",
       "      <th>total_acc_z_kurtosis</th>\n",
       "      <th>total_acc_z_energy</th>\n",
       "      <th>total_acc_z_entropy</th>\n",
       "      <th>total_acc_z_mean_power</th>\n",
       "      <th>total_acc_z_max_freq_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002269</td>\n",
       "      <td>0.002941</td>\n",
       "      <td>-0.004294</td>\n",
       "      <td>0.010810</td>\n",
       "      <td>0.002025</td>\n",
       "      <td>0.481111</td>\n",
       "      <td>-0.395797</td>\n",
       "      <td>0.226044</td>\n",
       "      <td>4.338396</td>\n",
       "      <td>0.024127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003970</td>\n",
       "      <td>0.088742</td>\n",
       "      <td>0.109485</td>\n",
       "      <td>0.099841</td>\n",
       "      <td>0.071125</td>\n",
       "      <td>0.493800</td>\n",
       "      <td>163.220498</td>\n",
       "      <td>1.654016</td>\n",
       "      <td>0.132356</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.001981</td>\n",
       "      <td>-0.006706</td>\n",
       "      <td>0.005251</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>-0.480776</td>\n",
       "      <td>1.472747</td>\n",
       "      <td>0.064786</td>\n",
       "      <td>4.462213</td>\n",
       "      <td>0.016851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004918</td>\n",
       "      <td>0.081100</td>\n",
       "      <td>0.105788</td>\n",
       "      <td>0.097748</td>\n",
       "      <td>-1.084209</td>\n",
       "      <td>1.257869</td>\n",
       "      <td>154.361101</td>\n",
       "      <td>1.850264</td>\n",
       "      <td>0.135705</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_acc_x_mean  body_acc_x_std  body_acc_x_min  body_acc_x_max  \\\n",
       "0         0.002269        0.002941       -0.004294        0.010810   \n",
       "1         0.000174        0.001981       -0.006706        0.005251   \n",
       "\n",
       "   body_acc_x_median  body_acc_x_skew  body_acc_x_kurtosis  body_acc_x_energy  \\\n",
       "0           0.002025         0.481111            -0.395797           0.226044   \n",
       "1           0.000110        -0.480776             1.472747           0.064786   \n",
       "\n",
       "   body_acc_x_entropy  body_acc_x_mean_power  ...  total_acc_z_std  \\\n",
       "0            4.338396               0.024127  ...         0.003970   \n",
       "1            4.462213               0.016851  ...         0.004918   \n",
       "\n",
       "   total_acc_z_min  total_acc_z_max  total_acc_z_median  total_acc_z_skew  \\\n",
       "0         0.088742         0.109485            0.099841          0.071125   \n",
       "1         0.081100         0.105788            0.097748         -1.084209   \n",
       "\n",
       "   total_acc_z_kurtosis  total_acc_z_energy  total_acc_z_entropy  \\\n",
       "0              0.493800          163.220498             1.654016   \n",
       "1              1.257869          154.361101             1.850264   \n",
       "\n",
       "   total_acc_z_mean_power  total_acc_z_max_freq_idx  \n",
       "0                0.132356                         0  \n",
       "1                0.135705                         0  \n",
       "\n",
       "[2 rows x 99 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da72bd85",
   "metadata": {},
   "source": [
    "## PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a3fdd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f54e31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# missing_values = df_features.isnull().sum()\n",
    "print(df_features.isnull().sum().sum())  # Should be 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1e22fc",
   "metadata": {},
   "source": [
    "if any found :\n",
    "    df_features.fillna(df_features.mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a487109a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_acc_x_mean</th>\n",
       "      <th>body_acc_x_std</th>\n",
       "      <th>body_acc_x_min</th>\n",
       "      <th>body_acc_x_max</th>\n",
       "      <th>body_acc_x_median</th>\n",
       "      <th>body_acc_x_skew</th>\n",
       "      <th>body_acc_x_kurtosis</th>\n",
       "      <th>body_acc_x_energy</th>\n",
       "      <th>body_acc_x_entropy</th>\n",
       "      <th>body_acc_x_mean_power</th>\n",
       "      <th>...</th>\n",
       "      <th>total_acc_z_std</th>\n",
       "      <th>total_acc_z_min</th>\n",
       "      <th>total_acc_z_max</th>\n",
       "      <th>total_acc_z_median</th>\n",
       "      <th>total_acc_z_skew</th>\n",
       "      <th>total_acc_z_kurtosis</th>\n",
       "      <th>total_acc_z_energy</th>\n",
       "      <th>total_acc_z_entropy</th>\n",
       "      <th>total_acc_z_mean_power</th>\n",
       "      <th>total_acc_z_max_freq_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002269</td>\n",
       "      <td>0.002941</td>\n",
       "      <td>-0.004294</td>\n",
       "      <td>0.010810</td>\n",
       "      <td>0.002025</td>\n",
       "      <td>0.481111</td>\n",
       "      <td>-0.395797</td>\n",
       "      <td>0.226044</td>\n",
       "      <td>4.338396</td>\n",
       "      <td>0.024127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003970</td>\n",
       "      <td>0.088742</td>\n",
       "      <td>0.109485</td>\n",
       "      <td>0.099841</td>\n",
       "      <td>0.071125</td>\n",
       "      <td>0.493800</td>\n",
       "      <td>163.220498</td>\n",
       "      <td>1.654016</td>\n",
       "      <td>0.132356</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.001981</td>\n",
       "      <td>-0.006706</td>\n",
       "      <td>0.005251</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>-0.480776</td>\n",
       "      <td>1.472747</td>\n",
       "      <td>0.064786</td>\n",
       "      <td>4.462213</td>\n",
       "      <td>0.016851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004918</td>\n",
       "      <td>0.081100</td>\n",
       "      <td>0.105788</td>\n",
       "      <td>0.097748</td>\n",
       "      <td>-1.084209</td>\n",
       "      <td>1.257869</td>\n",
       "      <td>154.361101</td>\n",
       "      <td>1.850264</td>\n",
       "      <td>0.135705</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_acc_x_mean  body_acc_x_std  body_acc_x_min  body_acc_x_max  \\\n",
       "0         0.002269        0.002941       -0.004294        0.010810   \n",
       "1         0.000174        0.001981       -0.006706        0.005251   \n",
       "\n",
       "   body_acc_x_median  body_acc_x_skew  body_acc_x_kurtosis  body_acc_x_energy  \\\n",
       "0           0.002025         0.481111            -0.395797           0.226044   \n",
       "1           0.000110        -0.480776             1.472747           0.064786   \n",
       "\n",
       "   body_acc_x_entropy  body_acc_x_mean_power  ...  total_acc_z_std  \\\n",
       "0            4.338396               0.024127  ...         0.003970   \n",
       "1            4.462213               0.016851  ...         0.004918   \n",
       "\n",
       "   total_acc_z_min  total_acc_z_max  total_acc_z_median  total_acc_z_skew  \\\n",
       "0         0.088742         0.109485            0.099841          0.071125   \n",
       "1         0.081100         0.105788            0.097748         -1.084209   \n",
       "\n",
       "   total_acc_z_kurtosis  total_acc_z_energy  total_acc_z_entropy  \\\n",
       "0              0.493800          163.220498             1.654016   \n",
       "1              1.257869          154.361101             1.850264   \n",
       "\n",
       "   total_acc_z_mean_power  total_acc_z_max_freq_idx  \n",
       "0                0.132356                         0  \n",
       "1                0.135705                         0  \n",
       "\n",
       "[2 rows x 99 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_features\n",
    "X.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4769cc9",
   "metadata": {},
   "source": [
    "STANDARDIZE FEATURES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1ba241",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert back to DataFrame with feature names\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d94dd1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_acc_x_mean</th>\n",
       "      <th>body_acc_x_std</th>\n",
       "      <th>body_acc_x_min</th>\n",
       "      <th>body_acc_x_max</th>\n",
       "      <th>body_acc_x_median</th>\n",
       "      <th>body_acc_x_skew</th>\n",
       "      <th>body_acc_x_kurtosis</th>\n",
       "      <th>body_acc_x_energy</th>\n",
       "      <th>body_acc_x_entropy</th>\n",
       "      <th>body_acc_x_mean_power</th>\n",
       "      <th>...</th>\n",
       "      <th>total_acc_z_std</th>\n",
       "      <th>total_acc_z_min</th>\n",
       "      <th>total_acc_z_max</th>\n",
       "      <th>total_acc_z_median</th>\n",
       "      <th>total_acc_z_skew</th>\n",
       "      <th>total_acc_z_kurtosis</th>\n",
       "      <th>total_acc_z_energy</th>\n",
       "      <th>total_acc_z_entropy</th>\n",
       "      <th>total_acc_z_mean_power</th>\n",
       "      <th>total_acc_z_max_freq_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.210534</td>\n",
       "      <td>-0.883335</td>\n",
       "      <td>0.918871</td>\n",
       "      <td>-0.868773</td>\n",
       "      <td>0.611526</td>\n",
       "      <td>0.356362</td>\n",
       "      <td>-0.339400</td>\n",
       "      <td>-0.706303</td>\n",
       "      <td>0.782367</td>\n",
       "      <td>-0.884585</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.913413</td>\n",
       "      <td>0.404670</td>\n",
       "      <td>-0.444325</td>\n",
       "      <td>0.039661</td>\n",
       "      <td>0.299175</td>\n",
       "      <td>0.052158</td>\n",
       "      <td>-0.563109</td>\n",
       "      <td>-0.730411</td>\n",
       "      <td>-1.203022</td>\n",
       "      <td>-0.287719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.060208</td>\n",
       "      <td>-0.890098</td>\n",
       "      <td>0.908664</td>\n",
       "      <td>-0.884263</td>\n",
       "      <td>0.567732</td>\n",
       "      <td>-1.179130</td>\n",
       "      <td>0.580954</td>\n",
       "      <td>-0.706492</td>\n",
       "      <td>1.339885</td>\n",
       "      <td>-0.893425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.900186</td>\n",
       "      <td>0.388502</td>\n",
       "      <td>-0.456851</td>\n",
       "      <td>0.033242</td>\n",
       "      <td>-1.522574</td>\n",
       "      <td>0.556553</td>\n",
       "      <td>-0.565740</td>\n",
       "      <td>-0.593033</td>\n",
       "      <td>-1.195720</td>\n",
       "      <td>-0.287719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_acc_x_mean  body_acc_x_std  body_acc_x_min  body_acc_x_max  \\\n",
       "0         0.210534       -0.883335        0.918871       -0.868773   \n",
       "1         0.060208       -0.890098        0.908664       -0.884263   \n",
       "\n",
       "   body_acc_x_median  body_acc_x_skew  body_acc_x_kurtosis  body_acc_x_energy  \\\n",
       "0           0.611526         0.356362            -0.339400          -0.706303   \n",
       "1           0.567732        -1.179130             0.580954          -0.706492   \n",
       "\n",
       "   body_acc_x_entropy  body_acc_x_mean_power  ...  total_acc_z_std  \\\n",
       "0            0.782367              -0.884585  ...        -0.913413   \n",
       "1            1.339885              -0.893425  ...        -0.900186   \n",
       "\n",
       "   total_acc_z_min  total_acc_z_max  total_acc_z_median  total_acc_z_skew  \\\n",
       "0         0.404670        -0.444325            0.039661          0.299175   \n",
       "1         0.388502        -0.456851            0.033242         -1.522574   \n",
       "\n",
       "   total_acc_z_kurtosis  total_acc_z_energy  total_acc_z_entropy  \\\n",
       "0              0.052158           -0.563109            -0.730411   \n",
       "1              0.556553           -0.565740            -0.593033   \n",
       "\n",
       "   total_acc_z_mean_power  total_acc_z_max_freq_idx  \n",
       "0               -1.203022                 -0.287719  \n",
       "1               -1.195720                 -0.287719  \n",
       "\n",
       "[2 rows x 99 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52f63b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_acc_x_mean</th>\n",
       "      <th>body_acc_x_std</th>\n",
       "      <th>body_acc_x_min</th>\n",
       "      <th>body_acc_x_max</th>\n",
       "      <th>body_acc_x_median</th>\n",
       "      <th>body_acc_x_skew</th>\n",
       "      <th>body_acc_x_kurtosis</th>\n",
       "      <th>body_acc_x_energy</th>\n",
       "      <th>body_acc_x_entropy</th>\n",
       "      <th>body_acc_x_mean_power</th>\n",
       "      <th>...</th>\n",
       "      <th>total_acc_z_std</th>\n",
       "      <th>total_acc_z_min</th>\n",
       "      <th>total_acc_z_max</th>\n",
       "      <th>total_acc_z_median</th>\n",
       "      <th>total_acc_z_skew</th>\n",
       "      <th>total_acc_z_kurtosis</th>\n",
       "      <th>total_acc_z_energy</th>\n",
       "      <th>total_acc_z_entropy</th>\n",
       "      <th>total_acc_z_mean_power</th>\n",
       "      <th>total_acc_z_max_freq_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.210534</td>\n",
       "      <td>-0.883335</td>\n",
       "      <td>0.918871</td>\n",
       "      <td>-0.868773</td>\n",
       "      <td>0.611526</td>\n",
       "      <td>0.356362</td>\n",
       "      <td>-0.339400</td>\n",
       "      <td>-0.706303</td>\n",
       "      <td>0.782367</td>\n",
       "      <td>-0.884585</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.913413</td>\n",
       "      <td>0.404670</td>\n",
       "      <td>-0.444325</td>\n",
       "      <td>0.039661</td>\n",
       "      <td>0.299175</td>\n",
       "      <td>0.052158</td>\n",
       "      <td>-0.563109</td>\n",
       "      <td>-0.730411</td>\n",
       "      <td>-1.203022</td>\n",
       "      <td>-0.287719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.060208</td>\n",
       "      <td>-0.890098</td>\n",
       "      <td>0.908664</td>\n",
       "      <td>-0.884263</td>\n",
       "      <td>0.567732</td>\n",
       "      <td>-1.179130</td>\n",
       "      <td>0.580954</td>\n",
       "      <td>-0.706492</td>\n",
       "      <td>1.339885</td>\n",
       "      <td>-0.893425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.900186</td>\n",
       "      <td>0.388502</td>\n",
       "      <td>-0.456851</td>\n",
       "      <td>0.033242</td>\n",
       "      <td>-1.522574</td>\n",
       "      <td>0.556553</td>\n",
       "      <td>-0.565740</td>\n",
       "      <td>-0.593033</td>\n",
       "      <td>-1.195720</td>\n",
       "      <td>-0.287719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_acc_x_mean  body_acc_x_std  body_acc_x_min  body_acc_x_max  \\\n",
       "0         0.210534       -0.883335        0.918871       -0.868773   \n",
       "1         0.060208       -0.890098        0.908664       -0.884263   \n",
       "\n",
       "   body_acc_x_median  body_acc_x_skew  body_acc_x_kurtosis  body_acc_x_energy  \\\n",
       "0           0.611526         0.356362            -0.339400          -0.706303   \n",
       "1           0.567732        -1.179130             0.580954          -0.706492   \n",
       "\n",
       "   body_acc_x_entropy  body_acc_x_mean_power  ...  total_acc_z_std  \\\n",
       "0            0.782367              -0.884585  ...        -0.913413   \n",
       "1            1.339885              -0.893425  ...        -0.900186   \n",
       "\n",
       "   total_acc_z_min  total_acc_z_max  total_acc_z_median  total_acc_z_skew  \\\n",
       "0         0.404670        -0.444325            0.039661          0.299175   \n",
       "1         0.388502        -0.456851            0.033242         -1.522574   \n",
       "\n",
       "   total_acc_z_kurtosis  total_acc_z_energy  total_acc_z_entropy  \\\n",
       "0              0.052158           -0.563109            -0.730411   \n",
       "1              0.556553           -0.565740            -0.593033   \n",
       "\n",
       "   total_acc_z_mean_power  total_acc_z_max_freq_idx  \n",
       "0               -1.203022                 -0.287719  \n",
       "1               -1.195720                 -0.287719  \n",
       "\n",
       "[2 rows x 99 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_CLEAN = X_scaled_df\n",
    "X_CLEAN.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230c0a78",
   "metadata": {},
   "source": [
    "## SAVING THE FILES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbfa26ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All files saved under: /Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/DATA_EXTRACTED_WITH_99_FEATURES\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define base path\n",
    "base_path = \"/Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/DATA_EXTRACTED_WITH_99_FEATURES\"\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "# Save cleaned feature data\n",
    "X_CLEAN.to_csv(os.path.join(base_path, \"X_CLEAN.csv\"), index=False)\n",
    "\n",
    "# Save labels\n",
    "y_df = pd.DataFrame(y, columns=['activity'])\n",
    "y_df.to_csv(os.path.join(base_path, \"Y_CLEAN.csv\"), index=False)\n",
    "\n",
    "# Save feature names\n",
    "feature_names = df_features.columns.tolist()\n",
    "with open(os.path.join(base_path, \"feature_names.txt\"), 'w') as f:\n",
    "    for name in feature_names:\n",
    "        f.write(f\"{name}\\n\")\n",
    "\n",
    "# Also save feature names in CSV\n",
    "feature_names_df = pd.DataFrame(feature_names, columns=['feature_name'])\n",
    "feature_names_df.to_csv(os.path.join(base_path, \"feature_names.csv\"), index=False)\n",
    "\n",
    "# Save shape info\n",
    "shape_info = {\n",
    "    'features_shape': df_features.shape,\n",
    "    'labels_shape': y_df.shape\n",
    "}\n",
    "shape_info_df = pd.DataFrame([shape_info])\n",
    "shape_info_df.to_csv(os.path.join(base_path, \"shape_info.csv\"), index=False)\n",
    "\n",
    "print(\" All files saved under:\", base_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6697251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README file created at: /Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE/README.txt\n"
     ]
    }
   ],
   "source": [
    "readme_text = \"\"\"\n",
    "FOLDER STRUCTURE AND CONTENTS\n",
    "============================================================\n",
    "\n",
    "/stage1_BASELINE/ \n",
    "│__DATA_EXTRACTED_WITH_99_FEATURES/ # Contains cleaned and preprocessed data\n",
    "│   ├── feature_names.csv\n",
    "│   ├── feature_names.txt\n",
    "│   ├── shape_info.csv\n",
    "│   ├── X_CLEAN.csv\n",
    "│   └── Y_CLEAN.csv\n",
    "|__OUTPUT/\n",
    "│   ├── <MODEL_NAME>_RESULTS/  # e.g., LinearSVC_RESULTS/\n",
    "│   │   ├── classification_report.txt\n",
    "│   │   └── confusion_matrix.png\n",
    "│   └── model_comparison.txt\n",
    "|__README.txt\n",
    "------------------------------------------------------------\n",
    "\n",
    "      \n",
    "      \n",
    "\n",
    "Folder: DATA_EXTRACTED_WITH_99_FEATURES\n",
    "------------------------------------------------------------\n",
    "This folder contains the cleaned and preprocessed data extracted from the **UCI HAR Dataset** using raw inertial signal files.\n",
    "\n",
    "✔️ SOURCE:\n",
    "-----------\n",
    "The raw data was taken from the Inertial Signals directory of the original UCI HAR dataset. These signals include 9 sensor signals:\n",
    "- body_acc_x\n",
    "- body_acc_y\n",
    "- body_acc_z\n",
    "- body_gyro_x\n",
    "- body_gyro_y\n",
    "- body_gyro_z\n",
    "- total_acc_x\n",
    "- total_acc_y\n",
    "- total_acc_z\n",
    "\n",
    "Each signal was recorded over 128 time steps for every activity window/sample.\n",
    "\n",
    "✔️ PIPELINE:\n",
    "------------\n",
    "1. **Raw Data Loaded**  \n",
    "   Inertial signal files from `train` and `test` directories were loaded and concatenated to form the full raw dataset.\n",
    "\n",
    "2. **Raw Files Saved**  \n",
    "   Raw signal data was saved as `X_RAW.csv` and labels as `Y_RAW.csv` for reference.\n",
    "\n",
    "3. **Feature Extraction**  \n",
    "   From each window (i.e., one sample of 128 time steps), the following statistical and frequency-domain features were extracted:\n",
    "   \n",
    "   For each signal (total 9), the following 11 features were computed:\n",
    "   - Mean\n",
    "   - Standard Deviation\n",
    "   - Minimum\n",
    "   - Maximum\n",
    "   - Median\n",
    "   - Skewness\n",
    "   - Kurtosis\n",
    "   - Energy (Sum of squares of FFT)\n",
    "   - Entropy (of normalized FFT)\n",
    "   - Mean Power (mean of FFT magnitudes)\n",
    "   - Max Frequency Index (argmax of FFT)\n",
    "\n",
    "   👉 This results in **99 features total** (9 signals × 11 features each).\n",
    "\n",
    "4. **Cleaned Data Saved**  \n",
    "   The extracted features were saved as:\n",
    "   - `X_CLEAN.csv` — Cleaned feature matrix (shape: [samples, 99])\n",
    "   - `Y_RAW.csv` — Corresponding labels for each sample\n",
    "   - `feature_names.csv` — Names of the 99 features\n",
    "   - `feature_names.txt` — Plain text list of all features\n",
    "   - `shape_info.csv` — Shape of the final feature and label datasets\n",
    "\n",
    "✔️ OUTPUT FILES:\n",
    "----------------\n",
    "- `X_CLEAN.csv` — Extracted feature dataset\n",
    "- `Y_RAW.csv` — Activity labels\n",
    "- `feature_names.csv` — Feature names in CSV\n",
    "- `feature_names.txt` — Feature names in plain text\n",
    "- `shape_info.csv` — Dataset shapes\n",
    "\n",
    "📌 This dataset is now ready for use in machine learning pipelines for Human Activity Recognition (HAR).\n",
    "\n",
    "Author: Priyam Pandey  \n",
    "Date: [24 TH MAY 2025]\n",
    "\n",
    "---------------------\n",
    "MODEL TRAINING AND EVALUATION\n",
    "------------------------------------------------------------\n",
    "* Loaded `X_CLEAN.csv` and `Y_CLEAN.csv` from the given base path\n",
    "\n",
    "* Split data into training and testing sets\n",
    "\n",
    "* Defined 12 classification models:\n",
    "  `[LinearSVC, GradientBoosting, ExtraTrees, Bagging, ANN, RandomForest, CART, GaussianNB, DecisionTree, AdaBoost, KNN, LogisticRegression]`\n",
    "\n",
    "* Trained each model using a loop\n",
    "\n",
    "* Calculated metrics: Accuracy, F1 Score, Recall, Precision\n",
    "* Saved classification report (`.txt`) and confusion matrix (`.png`) for each model in a separate folder named `<MODEL_NAME>_RESULTS`\n",
    "* Compiled all model scores into a comparison table, saved as `model_comparison.txt` in the base path\n",
    "------------------------------------------------------------\n",
    "Folder Structure:\n",
    "\n",
    "\n",
    "/stage1_BASELINE/\n",
    "│\n",
    "├── X_CLEAN.csv\n",
    "├── Y_CLEAN.csv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "base_path = \"/Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE\"\n",
    "with open(os.path.join(base_path, \"README.txt\"), \"w\") as f:\n",
    "    f.write(readme_text)\n",
    "print(\"README file created at:\", os.path.join(base_path, \"README.txt\"))\n",
    "# Save the README file in the base path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3bb53d",
   "metadata": {},
   "source": [
    "IMPORT LIBRAIES \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "528ff3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, recall_score, precision_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "# ML models\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier,\n",
    "    RandomForestClassifier, AdaBoostClassifier\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c7af1f",
   "metadata": {},
   "source": [
    "### LOADING DATA AND SPLITTING IT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20323e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base path\n",
    "base_path1 = \"/Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE/DATA_EXTRACTED_WITH_99_FEATURES\"\n",
    "\n",
    "# Load clean data\n",
    "X = pd.read_csv(os.path.join(base_path1, \"X_CLEAN.csv\"))\n",
    "y = pd.read_csv(os.path.join(base_path1, \"Y_CLEAN.csv\")).values.ravel()\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36169851",
   "metadata": {},
   "source": [
    "### DEFINING MODEL DIRECTORY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a1e8fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path= \"/Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE/OUTPUT1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4c7e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of models\n",
    "models = {\n",
    "    \"Linear_SVC\": LinearSVC(max_iter=10000),\n",
    "    \"Gradient_Boosting\": GradientBoostingClassifier(),\n",
    "    \"Extra_Trees\": ExtraTreesClassifier(),\n",
    "    \"Bagged_Decision_Trees\": BaggingClassifier(),\n",
    "    \"ANN\": MLPClassifier(max_iter=1000),\n",
    "    \"Random_Forest\": RandomForestClassifier(),\n",
    "    \"CART\": DecisionTreeClassifier(),  # Same as Decision Tree\n",
    "    \"Gaussian_Naive_Bayes\": GaussianNB(),\n",
    "    \"Decision_Tree\": DecisionTreeClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Logistic_Regression\": LogisticRegression(max_iter=10000)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2f8bf",
   "metadata": {},
   "source": [
    "### 🔁 Train, Evaluate, Save Report & Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f29b56db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Linear_SVC...\n",
      "Linear_SVC completed. Results saved in /Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE/OUTPUT1/Linear_SVC_RESULTS\n",
      "Training Gradient_Boosting...\n",
      "Gradient_Boosting completed. Results saved in /Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE/OUTPUT1/Gradient_Boosting_RESULTS\n",
      "Training Extra_Trees...\n",
      "Extra_Trees completed. Results saved in /Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE/OUTPUT1/Extra_Trees_RESULTS\n",
      "Training Bagged_Decision_Trees...\n",
      "Bagged_Decision_Trees completed. Results saved in /Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE/OUTPUT1/Bagged_Decision_Trees_RESULTS\n",
      "Training ANN...\n",
      "ANN completed. Results saved in /Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE/OUTPUT1/ANN_RESULTS\n",
      "Training Random_Forest...\n",
      "Random_Forest completed. Results saved in /Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE/OUTPUT1/Random_Forest_RESULTS\n",
      "Training CART...\n",
      "CART completed. Results saved in /Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE/OUTPUT1/CART_RESULTS\n",
      "Training Gaussian_Naive_Bayes...\n",
      "Gaussian_Naive_Bayes completed. Results saved in /Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE/OUTPUT1/Gaussian_Naive_Bayes_RESULTS\n",
      "Training Decision_Tree...\n",
      "Decision_Tree completed. Results saved in /Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE/OUTPUT1/Decision_Tree_RESULTS\n",
      "Training AdaBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost completed. Results saved in /Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE/OUTPUT1/AdaBoost_RESULTS\n",
      "Training KNN...\n",
      "KNN completed. Results saved in /Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE/OUTPUT1/KNN_RESULTS\n",
      "Training Logistic_Regression...\n",
      "Logistic_Regression completed. Results saved in /Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE/OUTPUT1/Logistic_Regression_RESULTS\n"
     ]
    }
   ],
   "source": [
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Loop through each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    results.append([model_name, acc, f1, recall, precision])\n",
    "\n",
    "    # Classification report & confusion matrix\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Save in model-specific folder\n",
    "    model_folder = os.path.join(base_path, f\"{model_name}_RESULTS\")\n",
    "    os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(model_folder, f\"{model_name}_classification_report.txt\"), \"w\") as f:\n",
    "        f.write(report)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.savefig(os.path.join(model_folder, f\"{model_name}_confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "    print(f\"{model_name} completed. Results saved in {model_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b6c8c8",
   "metadata": {},
   "source": [
    "### SAVE COMPARISON TABLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "524164fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All models trained and results saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create and sort comparison DataFrame\n",
    "comparison_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"F1 Score\", \"Recall\", \"Precision\"])\n",
    "comparison_df.sort_values(by=\"F1 Score\", ascending=False, inplace=True)\n",
    "\n",
    "# Save to TXT\n",
    "comparison_txt_path = os.path.join(base_path, \"model_comparison.txt\")\n",
    "with open(comparison_txt_path, \"w\") as f:\n",
    "    f.write(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"✅ All models trained and results saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdda58c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48da9518",
   "metadata": {},
   "source": [
    "# model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "434f57fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.19.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.3.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Using cached grpcio-1.71.0-cp312-cp312-macosx_10_14_universal2.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Using cached ml_dtypes-0.5.1-cp312-cp312-macosx_10_9_universal2.whl.metadata (21 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (13.7.1)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Downloading optree-0.16.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Using cached tensorflow-2.19.0-cp312-cp312-macosx_12_0_arm64.whl (252.7 MB)\n",
      "Downloading absl_py-2.3.0-py3-none-any.whl (135 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached grpcio-1.71.0-cp312-cp312-macosx_10_14_universal2.whl (11.3 MB)\n",
      "Downloading keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
      "Using cached ml_dtypes-0.5.1-cp312-cp312-macosx_10_9_universal2.whl (670 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.16.0-cp312-cp312-macosx_11_0_arm64.whl (341 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorboard-data-server, optree, opt-einsum, ml-dtypes, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow\n",
      "Successfully installed absl-py-2.3.0 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.71.0 keras-3.10.0 libclang-18.1.1 ml-dtypes-0.5.1 namex-0.1.0 opt-einsum-3.4.0 optree-0.16.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 termcolor-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c36c6ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Replace with your desired base path to save results\n",
    "base_path = \"/Users/priyam/paper_recreation/UCI_HAR_FEATURE_ANALYSIS_MODELLING/deep_learning_models_results\"\n",
    "\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb53044",
   "metadata": {},
   "source": [
    "### defining the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81210162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm(input_shape, n_classes):\n",
    "    return models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.LSTM(64),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.score_dense = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        score = tf.nn.softmax(self.score_dense(inputs), axis=1)\n",
    "        context = tf.reduce_sum(inputs * score, axis=1)\n",
    "        return context\n",
    "\n",
    "def get_attention_lstm(input_shape, n_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.LSTM(64, return_sequences=True)(inputs)\n",
    "    x = Attention()(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    outputs = layers.Dense(n_classes, activation='softmax')(x)\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "def get_bilstm(input_shape, n_classes):\n",
    "    return models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Bidirectional(layers.LSTM(64)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "def get_cnn_lstm(input_shape, n_classes):\n",
    "    return models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Conv1D(64, 3, activation='relu'),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.LSTM(64),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "def get_gru(input_shape, n_classes):\n",
    "    return models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.GRU(64),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "def get_transformer(input_shape, n_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Dense(64)(inputs)\n",
    "    x = layers.MultiHeadAttention(num_heads=2, key_dim=32)(x, x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    outputs = layers.Dense(n_classes, activation='softmax')(x)\n",
    "    return models.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "879ce7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🟡 Training LSTM\n",
      "Epoch 1/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - accuracy: 0.5330 - loss: 1.2663 - val_accuracy: 0.8811 - val_loss: 0.3775\n",
      "Epoch 2/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.9140 - loss: 0.2657 - val_accuracy: 0.9053 - val_loss: 0.2478\n",
      "Epoch 3/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - accuracy: 0.9405 - loss: 0.1715 - val_accuracy: 0.9126 - val_loss: 0.2251\n",
      "Epoch 4/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.9364 - loss: 0.1589 - val_accuracy: 0.9320 - val_loss: 0.1721\n",
      "Epoch 5/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.9419 - loss: 0.1385 - val_accuracy: 0.9442 - val_loss: 0.1434\n",
      "Epoch 6/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.9489 - loss: 0.1337 - val_accuracy: 0.9308 - val_loss: 0.1397\n",
      "Epoch 7/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.9394 - loss: 0.1438 - val_accuracy: 0.9333 - val_loss: 0.1626\n",
      "Epoch 8/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - accuracy: 0.9392 - loss: 0.1361 - val_accuracy: 0.9381 - val_loss: 0.1462\n",
      "Epoch 9/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - accuracy: 0.9466 - loss: 0.1228 - val_accuracy: 0.9357 - val_loss: 0.1703\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LSTM training complete. Results saved in: /Users/priyam/paper_recreation/UCI_HAR_FEATURE_ANALYSIS_MODELLING/deep_learning_models_results/LSTM\n",
      "\n",
      "🟡 Training Attention_LSTM\n",
      "Epoch 1/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 34ms/step - accuracy: 0.6229 - loss: 1.1440 - val_accuracy: 0.9248 - val_loss: 0.2138\n",
      "Epoch 2/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - accuracy: 0.9289 - loss: 0.1977 - val_accuracy: 0.9478 - val_loss: 0.1365\n",
      "Epoch 3/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - accuracy: 0.9393 - loss: 0.1421 - val_accuracy: 0.9430 - val_loss: 0.1382\n",
      "Epoch 4/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - accuracy: 0.9424 - loss: 0.1362 - val_accuracy: 0.9296 - val_loss: 0.1411\n",
      "Epoch 5/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - accuracy: 0.9521 - loss: 0.1182 - val_accuracy: 0.9405 - val_loss: 0.1298\n",
      "Epoch 6/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - accuracy: 0.9485 - loss: 0.1175 - val_accuracy: 0.9515 - val_loss: 0.1217\n",
      "Epoch 7/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.9507 - loss: 0.1135 - val_accuracy: 0.9515 - val_loss: 0.1215\n",
      "Epoch 8/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - accuracy: 0.9574 - loss: 0.1022 - val_accuracy: 0.9502 - val_loss: 0.1243\n",
      "Epoch 9/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - accuracy: 0.9585 - loss: 0.1053 - val_accuracy: 0.9515 - val_loss: 0.1198\n",
      "Epoch 10/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - accuracy: 0.9563 - loss: 0.1039 - val_accuracy: 0.9417 - val_loss: 0.1325\n",
      "Epoch 11/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - accuracy: 0.9576 - loss: 0.1041 - val_accuracy: 0.9515 - val_loss: 0.1132\n",
      "Epoch 12/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - accuracy: 0.9566 - loss: 0.0967 - val_accuracy: 0.9539 - val_loss: 0.1155\n",
      "Epoch 13/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - accuracy: 0.9562 - loss: 0.0975 - val_accuracy: 0.9527 - val_loss: 0.1116\n",
      "Epoch 14/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - accuracy: 0.9553 - loss: 0.1047 - val_accuracy: 0.9527 - val_loss: 0.1138\n",
      "Epoch 15/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.9580 - loss: 0.0986 - val_accuracy: 0.9515 - val_loss: 0.1093\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Attention_LSTM training complete. Results saved in: /Users/priyam/paper_recreation/UCI_HAR_FEATURE_ANALYSIS_MODELLING/deep_learning_models_results/Attention_LSTM\n",
      "\n",
      "🟡 Training BiLSTM\n",
      "Epoch 1/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - accuracy: 0.5855 - loss: 1.1343 - val_accuracy: 0.9248 - val_loss: 0.2244\n",
      "Epoch 2/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.9141 - loss: 0.2303 - val_accuracy: 0.9345 - val_loss: 0.1602\n",
      "Epoch 3/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 60ms/step - accuracy: 0.9397 - loss: 0.1507 - val_accuracy: 0.9454 - val_loss: 0.1373\n",
      "Epoch 4/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 61ms/step - accuracy: 0.9483 - loss: 0.1251 - val_accuracy: 0.9405 - val_loss: 0.1422\n",
      "Epoch 5/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 60ms/step - accuracy: 0.9465 - loss: 0.1239 - val_accuracy: 0.9454 - val_loss: 0.1269\n",
      "Epoch 6/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.9528 - loss: 0.1116 - val_accuracy: 0.9502 - val_loss: 0.1217\n",
      "Epoch 7/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.9548 - loss: 0.1151 - val_accuracy: 0.9478 - val_loss: 0.1254\n",
      "Epoch 8/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 59ms/step - accuracy: 0.9581 - loss: 0.1078 - val_accuracy: 0.9563 - val_loss: 0.1245\n",
      "Epoch 9/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.9525 - loss: 0.1242 - val_accuracy: 0.9393 - val_loss: 0.1515\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BiLSTM training complete. Results saved in: /Users/priyam/paper_recreation/UCI_HAR_FEATURE_ANALYSIS_MODELLING/deep_learning_models_results/BiLSTM\n",
      "\n",
      "🟡 Training CNN_LSTM\n",
      "Epoch 1/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.6313 - loss: 1.0112 - val_accuracy: 0.8968 - val_loss: 0.2791\n",
      "Epoch 2/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9142 - loss: 0.2308 - val_accuracy: 0.9114 - val_loss: 0.2216\n",
      "Epoch 3/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9431 - loss: 0.1506 - val_accuracy: 0.9333 - val_loss: 0.1503\n",
      "Epoch 4/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9475 - loss: 0.1236 - val_accuracy: 0.9466 - val_loss: 0.1306\n",
      "Epoch 5/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9450 - loss: 0.1257 - val_accuracy: 0.9430 - val_loss: 0.1392\n",
      "Epoch 6/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9556 - loss: 0.1081 - val_accuracy: 0.9430 - val_loss: 0.1371\n",
      "Epoch 7/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9488 - loss: 0.1246 - val_accuracy: 0.9442 - val_loss: 0.1277\n",
      "Epoch 8/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.9527 - loss: 0.1130 - val_accuracy: 0.9466 - val_loss: 0.1187\n",
      "Epoch 9/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9569 - loss: 0.1040 - val_accuracy: 0.9490 - val_loss: 0.1132\n",
      "Epoch 10/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9595 - loss: 0.0963 - val_accuracy: 0.9502 - val_loss: 0.1151\n",
      "Epoch 11/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.9555 - loss: 0.1098 - val_accuracy: 0.9527 - val_loss: 0.1217\n",
      "Epoch 12/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9572 - loss: 0.1010 - val_accuracy: 0.9539 - val_loss: 0.1126\n",
      "Epoch 13/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9602 - loss: 0.0928 - val_accuracy: 0.9527 - val_loss: 0.1139\n",
      "Epoch 14/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9527 - loss: 0.1010 - val_accuracy: 0.9502 - val_loss: 0.1143\n",
      "Epoch 15/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9555 - loss: 0.1062 - val_accuracy: 0.9515 - val_loss: 0.1126\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CNN_LSTM training complete. Results saved in: /Users/priyam/paper_recreation/UCI_HAR_FEATURE_ANALYSIS_MODELLING/deep_learning_models_results/CNN_LSTM\n",
      "\n",
      "🟡 Training GRU\n",
      "Epoch 1/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - accuracy: 0.5048 - loss: 1.3255 - val_accuracy: 0.7925 - val_loss: 0.5509\n",
      "Epoch 2/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - accuracy: 0.8414 - loss: 0.4366 - val_accuracy: 0.8883 - val_loss: 0.2977\n",
      "Epoch 3/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - accuracy: 0.8823 - loss: 0.2950 - val_accuracy: 0.9248 - val_loss: 0.1950\n",
      "Epoch 4/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.9315 - loss: 0.1690 - val_accuracy: 0.9357 - val_loss: 0.1633\n",
      "Epoch 5/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - accuracy: 0.9411 - loss: 0.1455 - val_accuracy: 0.9345 - val_loss: 0.1605\n",
      "Epoch 6/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.9424 - loss: 0.1394 - val_accuracy: 0.9381 - val_loss: 0.1553\n",
      "Epoch 7/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.9458 - loss: 0.1221 - val_accuracy: 0.9381 - val_loss: 0.1588\n",
      "Epoch 8/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - accuracy: 0.9455 - loss: 0.1240 - val_accuracy: 0.9466 - val_loss: 0.1492\n",
      "Epoch 9/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.9480 - loss: 0.1191 - val_accuracy: 0.9405 - val_loss: 0.1515\n",
      "Epoch 10/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.9477 - loss: 0.1267 - val_accuracy: 0.9454 - val_loss: 0.1534\n",
      "Epoch 11/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.9479 - loss: 0.1233 - val_accuracy: 0.9478 - val_loss: 0.1411\n",
      "Epoch 12/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.9565 - loss: 0.1086 - val_accuracy: 0.9490 - val_loss: 0.1406\n",
      "Epoch 13/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.9521 - loss: 0.1194 - val_accuracy: 0.9430 - val_loss: 0.1454\n",
      "Epoch 14/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.9512 - loss: 0.1162 - val_accuracy: 0.9490 - val_loss: 0.1379\n",
      "Epoch 15/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 36ms/step - accuracy: 0.9514 - loss: 0.1105 - val_accuracy: 0.9502 - val_loss: 0.1370\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GRU training complete. Results saved in: /Users/priyam/paper_recreation/UCI_HAR_FEATURE_ANALYSIS_MODELLING/deep_learning_models_results/GRU\n",
      "\n",
      "🟡 Training Transformer\n",
      "Epoch 1/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.5570 - loss: 1.1594 - val_accuracy: 0.8677 - val_loss: 0.3338\n",
      "Epoch 2/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 0.8715 - loss: 0.3206 - val_accuracy: 0.9041 - val_loss: 0.2417\n",
      "Epoch 3/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.9031 - loss: 0.2544 - val_accuracy: 0.9150 - val_loss: 0.2032\n",
      "Epoch 4/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.9124 - loss: 0.2198 - val_accuracy: 0.9260 - val_loss: 0.1845\n",
      "Epoch 5/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 0.9330 - loss: 0.1669 - val_accuracy: 0.9369 - val_loss: 0.1506\n",
      "Epoch 6/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 0.9317 - loss: 0.1640 - val_accuracy: 0.9393 - val_loss: 0.1499\n",
      "Epoch 7/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 42ms/step - accuracy: 0.9382 - loss: 0.1518 - val_accuracy: 0.9369 - val_loss: 0.1512\n",
      "Epoch 8/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - accuracy: 0.9350 - loss: 0.1556 - val_accuracy: 0.9417 - val_loss: 0.1458\n",
      "Epoch 9/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - accuracy: 0.9524 - loss: 0.1219 - val_accuracy: 0.9515 - val_loss: 0.1281\n",
      "Epoch 10/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - accuracy: 0.9465 - loss: 0.1296 - val_accuracy: 0.9417 - val_loss: 0.1460\n",
      "Epoch 11/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.9498 - loss: 0.1223 - val_accuracy: 0.9405 - val_loss: 0.1556\n",
      "Epoch 12/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - accuracy: 0.9504 - loss: 0.1187 - val_accuracy: 0.9381 - val_loss: 0.1608\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Transformer training complete. Results saved in: /Users/priyam/paper_recreation/UCI_HAR_FEATURE_ANALYSIS_MODELLING/deep_learning_models_results/Transformer\n"
     ]
    }
   ],
   "source": [
    "for model_name, model_func in model_factories.items():\n",
    "    print(f\"\\n🟡 Training {model_name}\")\n",
    "    \n",
    "    model = model_func(input_shape, n_classes)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Callbacks\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        epochs=15,\n",
    "        batch_size=64,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # 🔍 Evaluate\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "    # 📁 Save folder\n",
    "    model_dir = os.path.join(base_path, model_name)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # 📊 Save classification report\n",
    "    # 📊 Save classification report as CSV\n",
    "    report = classification_report(y_true, y_pred_classes, output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    report_df.to_csv(os.path.join(model_dir, \"classification_report.csv\"))\n",
    "    \n",
    "    # 📸 Also save classification report as PNG\n",
    "    fig, ax = plt.subplots(figsize=(10, len(report_df)*0.5 + 1))\n",
    "    ax.axis('off')\n",
    "    table = ax.table(cellText=np.round(report_df.values, 2),\n",
    "                    colLabels=report_df.columns,\n",
    "                    rowLabels=report_df.index,\n",
    "                    loc='center',\n",
    "                    cellLoc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 1.5)\n",
    "    \n",
    "    plt.title(f\"{model_name} Classification Report\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(model_dir, \"classification_report.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    # 🧩 Save confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred_classes)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f\"{model_name} Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(model_dir, \"confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # 💾 Save model\n",
    "    model.save(os.path.join(model_dir, f\"{model_name}.h5\"))\n",
    "    print(f\"✅ {model_name} training complete. Results saved in: {model_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b2de6c",
   "metadata": {},
   "source": [
    "### preparing dataset and model dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da04a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y_encoded should already be normalized and one-hot encoded\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "input_shape = x_train.shape[1:]\n",
    "n_classes = y_train.shape[1]\n",
    "\n",
    "model_factories = {\n",
    "    \"LSTM\": get_lstm,\n",
    "    \"Attention_LSTM\": get_attention_lstm,\n",
    "    \"BiLSTM\": get_bilstm,\n",
    "    \"CNN_LSTM\": get_cnn_lstm,\n",
    "    \"GRU\": get_gru,\n",
    "    \"Transformer\": get_transformer\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da47e844",
   "metadata": {},
   "source": [
    "### train save and evaluate models \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d350acbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🟡 Training LSTM\n",
      "Epoch 1/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step - accuracy: 0.5133 - loss: 1.2729 - val_accuracy: 0.8422 - val_loss: 0.3887\n",
      "Epoch 2/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.8685 - loss: 0.3384 - val_accuracy: 0.9284 - val_loss: 0.2258\n",
      "Epoch 3/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.9335 - loss: 0.1874 - val_accuracy: 0.9393 - val_loss: 0.1678\n",
      "Epoch 4/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - accuracy: 0.9501 - loss: 0.1380 - val_accuracy: 0.9430 - val_loss: 0.1568\n",
      "Epoch 5/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - accuracy: 0.9497 - loss: 0.1353 - val_accuracy: 0.9163 - val_loss: 0.1857\n",
      "Epoch 6/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.9447 - loss: 0.1477 - val_accuracy: 0.9296 - val_loss: 0.1979\n",
      "Epoch 7/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.9499 - loss: 0.1273 - val_accuracy: 0.9417 - val_loss: 0.1524\n",
      "Epoch 8/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.9545 - loss: 0.1156 - val_accuracy: 0.9466 - val_loss: 0.1410\n",
      "Epoch 9/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.9516 - loss: 0.1167 - val_accuracy: 0.9430 - val_loss: 0.1407\n",
      "Epoch 10/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.9554 - loss: 0.1125 - val_accuracy: 0.9235 - val_loss: 0.2090\n",
      "Epoch 11/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - accuracy: 0.9375 - loss: 0.1634 - val_accuracy: 0.9369 - val_loss: 0.1505\n",
      "Epoch 12/15\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.9439 - loss: 0.1437 - val_accuracy: 0.9430 - val_loss: 0.1565\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "              precision    recall  f1-score      support\n",
      "0              0.991914  0.997290  0.994595   369.000000\n",
      "1              0.992620  0.996296  0.994455   270.000000\n",
      "2              1.000000  0.992958  0.996466   284.000000\n",
      "3              0.811456  0.901857  0.854271   377.000000\n",
      "4              0.884244  0.776836  0.827068   354.000000\n",
      "5              1.000000  1.000000  1.000000   406.000000\n",
      "accuracy       0.941748  0.941748  0.941748     0.941748\n",
      "macro avg      0.946706  0.944206  0.944476  2060.000000\n",
      "weighted avg   0.943187  0.941748  0.941431  2060.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LSTM training complete. Results saved in: /Users/priyam/paper_recreation/UCI_HAR_FEATURE_ANALYSIS_MODELLING/deep_learning_models_results/LSTM\n",
      "\n",
      "🟡 Training Attention_LSTM\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Attention.call().\n\n\u001b[1mtf.function only supports singleton tf.Variables created on the first call. Make sure the tf.Variable is only created once or created outside tf.function. See https://www.tensorflow.org/guide/function#creating_tfvariables for more information.\u001b[0m\n\nArguments received by Attention.call():\n  • inputs=tf.Tensor(shape=(None, 128, 64), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Callbacks\u001b[39;00m\n\u001b[1;32m      8\u001b[0m early_stop \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 10\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     11\u001b[0m     x_train, y_train,\n\u001b[1;32m     12\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m,\n\u001b[1;32m     13\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m     14\u001b[0m     validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m     15\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[early_stop],\n\u001b[1;32m     16\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 🔍 Evaluate\u001b[39;00m\n\u001b[1;32m     20\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_test)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[29], line 11\u001b[0m, in \u001b[0;36mAttention.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m---> 11\u001b[0m     score \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m1\u001b[39m)(inputs), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m     context \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_sum(inputs \u001b[38;5;241m*\u001b[39m score, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m context\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Attention.call().\n\n\u001b[1mtf.function only supports singleton tf.Variables created on the first call. Make sure the tf.Variable is only created once or created outside tf.function. See https://www.tensorflow.org/guide/function#creating_tfvariables for more information.\u001b[0m\n\nArguments received by Attention.call():\n  • inputs=tf.Tensor(shape=(None, 128, 64), dtype=float32)"
     ]
    }
   ],
   "source": [
    "for model_name, model_func in model_factories.items():\n",
    "    print(f\"\\n🟡 Training {model_name}\")\n",
    "    \n",
    "    model = model_func(input_shape, n_classes)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Callbacks\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        epochs=15,\n",
    "        batch_size=64,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # 🔍 Evaluate\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "    # 📁 Save folder\n",
    "    model_dir = os.path.join(base_path, model_name)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # 📊 Save classification report\n",
    "    # 📊 Save classification report as CSV\n",
    "    report = classification_report(y_true, y_pred_classes, output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    report_df.to_csv(os.path.join(model_dir, \"classification_report.csv\"))\n",
    "    \n",
    "    # 📸 Also save classification report as PNG\n",
    "    fig, ax = plt.subplots(figsize=(10, len(report_df)*0.5 + 1))\n",
    "    ax.axis('off')\n",
    "    table = ax.table(cellText=np.round(report_df.values, 2),\n",
    "                    colLabels=report_df.columns,\n",
    "                    rowLabels=report_df.index,\n",
    "                    loc='center',\n",
    "                    cellLoc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 1.5)\n",
    "    \n",
    "    plt.title(f\"{model_name} Classification Report\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(model_dir, \"classification_report.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    # 🧩 Save confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred_classes)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f\"{model_name} Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(model_dir, \"confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # 💾 Save model\n",
    "    model.save(os.path.join(model_dir, f\"{model_name}.h5\"))\n",
    "    print(f\"✅ {model_name} training complete. Results saved in: {model_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0fce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_true, y_pred_classes, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "report_df.to_csv(os.path.join(model_dir, \"classification_report.csv\"))\n",
    "print(report_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
