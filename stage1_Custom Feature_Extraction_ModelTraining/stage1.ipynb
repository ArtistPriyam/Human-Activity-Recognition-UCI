{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fde8e83",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7430c013",
   "metadata": {},
   "source": [
    "## FEW CONCEPTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b16bf4",
   "metadata": {},
   "source": [
    "### what mean by timestamps and inertial signals ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5072dfd2",
   "metadata": {},
   "source": [
    "\n",
    "Inertial signals are measurements from sensors like accelerometers and gyroscopes that capture motion‚Äîspecifically linear acceleration and angular velocity. In the UCI HAR dataset, these include signals such as body acceleration, gyroscope data, and total acceleration along the X, Y, and Z axes. That gives a total of 9 signals.\n",
    "\n",
    "Time stamps refer to the sequence of measurements taken over time. The dataset is divided into windows of 2.56 seconds, sampled at 50 Hz, meaning each window contains 128 time steps. So, for each sample, you have 128 values for each of the 9 signals, forming a matrix of shape (128, 9) per sample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5b68fb",
   "metadata": {},
   "source": [
    "### reason for shape ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a514e1c7",
   "metadata": {},
   "source": [
    "\n",
    "the shape `(10299, 128, 9)` is **correct** for the UCI HAR Dataset **Inertial Signals** when processed properly. Here's what each dimension means:\n",
    "\n",
    "* `10299`: Total number of samples (train + test combined: 7352 train + 2947 test)\n",
    "* `128`: Each sample is a time window of 128 readings (time steps)\n",
    "* `9`: There are **9 inertial signal features**:\n",
    "  * Total Acc: `body_acc_x/y/z`\n",
    "  * Body Acc: `body_gyro_x/y/z`\n",
    "  * Jerk: `total_acc_x/y/z`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f31337",
   "metadata": {},
   "source": [
    "### üìä Why the **train_data has 561 features**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448bdc59",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- The 561 features come from **hand-crafted feature extraction** on the raw signal data.\n",
    "‚öôÔ∏è Step-by-step:\n",
    "\n",
    "1. **Raw data shape** (after stacking `Inertial Signals`):\n",
    "\n",
    "   * Shape = `(7352, 128, 9)` for training\n",
    "   * Raw signal from accelerometer + gyroscope in time windows.\n",
    "\n",
    "2. **Feature extraction process** (already done in the dataset's `X_train.txt` file):\n",
    "\n",
    "   * From each 128-sample √ó 9-signal window ‚Üí extract statistical features:\n",
    "\n",
    "     * e.g., mean, std, energy, entropy, correlation, FFT coefficients, etc.\n",
    "     * This is done per signal.\n",
    "   * In total, **561 features per sample** are engineered this way.\n",
    "\n",
    "3. **Final train dataset used in classification:**\n",
    "\n",
    "   * Shape = `(7352, 561)`\n",
    "   * Each row is a sample (1 time window)\n",
    "   * Each column is a feature\n",
    " üìÅ Files Involved:\n",
    "\n",
    "* `Inertial Signals/` ‚Üí Raw 128√ó9 time series\n",
    "* `X_train.txt` ‚Üí Preprocessed data with 561 features\n",
    "* `y_train.txt` ‚Üí Corresponding activity labels\n",
    "\n",
    "If we're working from raw Inertial Signals, you'll have to **re-implement feature extraction** to get the 561 features ‚Äî or use the `X_train.txt` directly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cb54f4",
   "metadata": {},
   "source": [
    "# concatatanating the signal files to create raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0046410d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e6bb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_40803/3631378305.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_40803/3631378305.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_40803/3631378305.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_40803/3631378305.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_40803/3631378305.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_40803/3631378305.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_40803/3631378305.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_40803/3631378305.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_40803/3631378305.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_40803/3631378305.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_40803/3631378305.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_40803/3631378305.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_40803/3631378305.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_40803/3631378305.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_40803/3631378305.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_40803/3631378305.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_40803/3631378305.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_40803/3631378305.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_signals(signal_dir):\n",
    "    filenames = sorted(os.listdir(signal_dir))\n",
    "    signal_data = [pd.read_csv(os.path.join(signal_dir, f), delim_whitespace=True, header=None) \n",
    "                   for f in filenames]\n",
    "    return np.stack(signal_data, axis=-1)  # shape: (samples, time, features)\n",
    "\n",
    "x_train_raw = load_signals(\"/Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/UCI HAR Dataset/train/Inertial Signals\")\n",
    "\n",
    "x_test_raw = load_signals(\"/Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/UCI HAR Dataset/test/Inertial Signals\")\n",
    "# Combine train and test data\n",
    "x_raw = np.concatenate((x_train_raw, x_test_raw), axis=0)\n",
    "y_train = pd.read_csv(\"/Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/UCI HAR Dataset/train/y_train.txt\", header=None).values.flatten()\n",
    "y_test = pd.read_csv(\"/Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/UCI HAR Dataset/test/y_test.txt\", header=None).values.flatten()\n",
    "# Combine train and test labels\n",
    "y = np.concatenate((y_train, y_test), axis=0)\n",
    "# convert x_raw to a DataFrame without flattening\n",
    "# x = pd.DataFrame(x_raw)  # shape: (samples, time * features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f576b99",
   "metadata": {},
   "source": [
    "# FEATURE EXTRACTION FROM RAW_FILE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d8b745f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "from scipy.fft import fft\n",
    "\n",
    "def extract_features_from_window(window):\n",
    "    features = []\n",
    "    for i, signal in enumerate(window.T):  # window.T shape (9, 128) ‚Üí iterate over 9 signals\n",
    "        # Time-domain features\n",
    "        features.append(np.mean(signal))\n",
    "        features.append(np.std(signal))\n",
    "        features.append(np.min(signal))\n",
    "        features.append(np.max(signal))\n",
    "        features.append(np.median(signal))\n",
    "        features.append(skew(signal))\n",
    "        features.append(kurtosis(signal))\n",
    "\n",
    "        # Frequency-domain features\n",
    "        fft_vals = np.abs(fft(signal))\n",
    "        fft_norm = fft_vals / (np.sum(fft_vals) + 1e-12)  # avoid div zero\n",
    "        features.append(np.sum(fft_vals**2))              # Energy\n",
    "        features.append(entropy(fft_norm + 1e-12))        # Entropy\n",
    "        features.append(np.mean(fft_vals))                 # Mean power\n",
    "        features.append(np.argmax(fft_vals))               # Max freq index (dominant frequency)\n",
    "    return features\n",
    "\n",
    "def extract_features_from_all_windows(x_raw):\n",
    "    feature_names = []\n",
    "    signals = ['body_acc_x', 'body_acc_y', 'body_acc_z',\n",
    "               'body_gyro_x', 'body_gyro_y', 'body_gyro_z',\n",
    "               'total_acc_x', 'total_acc_y', 'total_acc_z']\n",
    "    stats = ['mean', 'std', 'min', 'max', 'median', 'skew', 'kurtosis',\n",
    "             'energy', 'entropy', 'mean_power', 'max_freq_idx']\n",
    "    \n",
    "    for sig in signals:\n",
    "        for stat in stats:\n",
    "            feature_names.append(f\"{sig}_{stat}\")\n",
    "    \n",
    "    all_features = []\n",
    "    for window in x_raw:\n",
    "        feats = extract_features_from_window(window)\n",
    "        all_features.append(feats)\n",
    "        \n",
    "    df_features = pd.DataFrame(all_features, columns=feature_names)\n",
    "    return df_features\n",
    "\n",
    "# Usage:\n",
    "# x_features_df = extract_features_from_all_windows(x_raw)\n",
    "# print(x_features_df.shape)  # (samples, 9*11=99 features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75f439fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = extract_features_from_all_windows(x_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "092849cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_acc_x_mean</th>\n",
       "      <th>body_acc_x_std</th>\n",
       "      <th>body_acc_x_min</th>\n",
       "      <th>body_acc_x_max</th>\n",
       "      <th>body_acc_x_median</th>\n",
       "      <th>body_acc_x_skew</th>\n",
       "      <th>body_acc_x_kurtosis</th>\n",
       "      <th>body_acc_x_energy</th>\n",
       "      <th>body_acc_x_entropy</th>\n",
       "      <th>body_acc_x_mean_power</th>\n",
       "      <th>...</th>\n",
       "      <th>total_acc_z_std</th>\n",
       "      <th>total_acc_z_min</th>\n",
       "      <th>total_acc_z_max</th>\n",
       "      <th>total_acc_z_median</th>\n",
       "      <th>total_acc_z_skew</th>\n",
       "      <th>total_acc_z_kurtosis</th>\n",
       "      <th>total_acc_z_energy</th>\n",
       "      <th>total_acc_z_entropy</th>\n",
       "      <th>total_acc_z_mean_power</th>\n",
       "      <th>total_acc_z_max_freq_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002269</td>\n",
       "      <td>0.002941</td>\n",
       "      <td>-0.004294</td>\n",
       "      <td>0.010810</td>\n",
       "      <td>0.002025</td>\n",
       "      <td>0.481111</td>\n",
       "      <td>-0.395797</td>\n",
       "      <td>0.226044</td>\n",
       "      <td>4.338396</td>\n",
       "      <td>0.024127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003970</td>\n",
       "      <td>0.088742</td>\n",
       "      <td>0.109485</td>\n",
       "      <td>0.099841</td>\n",
       "      <td>0.071125</td>\n",
       "      <td>0.493800</td>\n",
       "      <td>163.220498</td>\n",
       "      <td>1.654016</td>\n",
       "      <td>0.132356</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.001981</td>\n",
       "      <td>-0.006706</td>\n",
       "      <td>0.005251</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>-0.480776</td>\n",
       "      <td>1.472747</td>\n",
       "      <td>0.064786</td>\n",
       "      <td>4.462213</td>\n",
       "      <td>0.016851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004918</td>\n",
       "      <td>0.081100</td>\n",
       "      <td>0.105788</td>\n",
       "      <td>0.097748</td>\n",
       "      <td>-1.084209</td>\n",
       "      <td>1.257869</td>\n",
       "      <td>154.361101</td>\n",
       "      <td>1.850264</td>\n",
       "      <td>0.135705</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows √ó 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_acc_x_mean  body_acc_x_std  body_acc_x_min  body_acc_x_max  \\\n",
       "0         0.002269        0.002941       -0.004294        0.010810   \n",
       "1         0.000174        0.001981       -0.006706        0.005251   \n",
       "\n",
       "   body_acc_x_median  body_acc_x_skew  body_acc_x_kurtosis  body_acc_x_energy  \\\n",
       "0           0.002025         0.481111            -0.395797           0.226044   \n",
       "1           0.000110        -0.480776             1.472747           0.064786   \n",
       "\n",
       "   body_acc_x_entropy  body_acc_x_mean_power  ...  total_acc_z_std  \\\n",
       "0            4.338396               0.024127  ...         0.003970   \n",
       "1            4.462213               0.016851  ...         0.004918   \n",
       "\n",
       "   total_acc_z_min  total_acc_z_max  total_acc_z_median  total_acc_z_skew  \\\n",
       "0         0.088742         0.109485            0.099841          0.071125   \n",
       "1         0.081100         0.105788            0.097748         -1.084209   \n",
       "\n",
       "   total_acc_z_kurtosis  total_acc_z_energy  total_acc_z_entropy  \\\n",
       "0              0.493800          163.220498             1.654016   \n",
       "1              1.257869          154.361101             1.850264   \n",
       "\n",
       "   total_acc_z_mean_power  total_acc_z_max_freq_idx  \n",
       "0                0.132356                         0  \n",
       "1                0.135705                         0  \n",
       "\n",
       "[2 rows x 99 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da72bd85",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a3fdd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f54e31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# missing_values = df_features.isnull().sum()\n",
    "print(df_features.isnull().sum().sum())  # Should be 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1e22fc",
   "metadata": {},
   "source": [
    "if any found :\n",
    "    df_features.fillna(df_features.mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a487109a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_acc_x_mean</th>\n",
       "      <th>body_acc_x_std</th>\n",
       "      <th>body_acc_x_min</th>\n",
       "      <th>body_acc_x_max</th>\n",
       "      <th>body_acc_x_median</th>\n",
       "      <th>body_acc_x_skew</th>\n",
       "      <th>body_acc_x_kurtosis</th>\n",
       "      <th>body_acc_x_energy</th>\n",
       "      <th>body_acc_x_entropy</th>\n",
       "      <th>body_acc_x_mean_power</th>\n",
       "      <th>...</th>\n",
       "      <th>total_acc_z_std</th>\n",
       "      <th>total_acc_z_min</th>\n",
       "      <th>total_acc_z_max</th>\n",
       "      <th>total_acc_z_median</th>\n",
       "      <th>total_acc_z_skew</th>\n",
       "      <th>total_acc_z_kurtosis</th>\n",
       "      <th>total_acc_z_energy</th>\n",
       "      <th>total_acc_z_entropy</th>\n",
       "      <th>total_acc_z_mean_power</th>\n",
       "      <th>total_acc_z_max_freq_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002269</td>\n",
       "      <td>0.002941</td>\n",
       "      <td>-0.004294</td>\n",
       "      <td>0.010810</td>\n",
       "      <td>0.002025</td>\n",
       "      <td>0.481111</td>\n",
       "      <td>-0.395797</td>\n",
       "      <td>0.226044</td>\n",
       "      <td>4.338396</td>\n",
       "      <td>0.024127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003970</td>\n",
       "      <td>0.088742</td>\n",
       "      <td>0.109485</td>\n",
       "      <td>0.099841</td>\n",
       "      <td>0.071125</td>\n",
       "      <td>0.493800</td>\n",
       "      <td>163.220498</td>\n",
       "      <td>1.654016</td>\n",
       "      <td>0.132356</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.001981</td>\n",
       "      <td>-0.006706</td>\n",
       "      <td>0.005251</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>-0.480776</td>\n",
       "      <td>1.472747</td>\n",
       "      <td>0.064786</td>\n",
       "      <td>4.462213</td>\n",
       "      <td>0.016851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004918</td>\n",
       "      <td>0.081100</td>\n",
       "      <td>0.105788</td>\n",
       "      <td>0.097748</td>\n",
       "      <td>-1.084209</td>\n",
       "      <td>1.257869</td>\n",
       "      <td>154.361101</td>\n",
       "      <td>1.850264</td>\n",
       "      <td>0.135705</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows √ó 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_acc_x_mean  body_acc_x_std  body_acc_x_min  body_acc_x_max  \\\n",
       "0         0.002269        0.002941       -0.004294        0.010810   \n",
       "1         0.000174        0.001981       -0.006706        0.005251   \n",
       "\n",
       "   body_acc_x_median  body_acc_x_skew  body_acc_x_kurtosis  body_acc_x_energy  \\\n",
       "0           0.002025         0.481111            -0.395797           0.226044   \n",
       "1           0.000110        -0.480776             1.472747           0.064786   \n",
       "\n",
       "   body_acc_x_entropy  body_acc_x_mean_power  ...  total_acc_z_std  \\\n",
       "0            4.338396               0.024127  ...         0.003970   \n",
       "1            4.462213               0.016851  ...         0.004918   \n",
       "\n",
       "   total_acc_z_min  total_acc_z_max  total_acc_z_median  total_acc_z_skew  \\\n",
       "0         0.088742         0.109485            0.099841          0.071125   \n",
       "1         0.081100         0.105788            0.097748         -1.084209   \n",
       "\n",
       "   total_acc_z_kurtosis  total_acc_z_energy  total_acc_z_entropy  \\\n",
       "0              0.493800          163.220498             1.654016   \n",
       "1              1.257869          154.361101             1.850264   \n",
       "\n",
       "   total_acc_z_mean_power  total_acc_z_max_freq_idx  \n",
       "0                0.132356                         0  \n",
       "1                0.135705                         0  \n",
       "\n",
       "[2 rows x 99 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_features\n",
    "X.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4769cc9",
   "metadata": {},
   "source": [
    "STANDARDIZE FEATURES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1ba241",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert back to DataFrame with feature names\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d94dd1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_acc_x_mean</th>\n",
       "      <th>body_acc_x_std</th>\n",
       "      <th>body_acc_x_min</th>\n",
       "      <th>body_acc_x_max</th>\n",
       "      <th>body_acc_x_median</th>\n",
       "      <th>body_acc_x_skew</th>\n",
       "      <th>body_acc_x_kurtosis</th>\n",
       "      <th>body_acc_x_energy</th>\n",
       "      <th>body_acc_x_entropy</th>\n",
       "      <th>body_acc_x_mean_power</th>\n",
       "      <th>...</th>\n",
       "      <th>total_acc_z_std</th>\n",
       "      <th>total_acc_z_min</th>\n",
       "      <th>total_acc_z_max</th>\n",
       "      <th>total_acc_z_median</th>\n",
       "      <th>total_acc_z_skew</th>\n",
       "      <th>total_acc_z_kurtosis</th>\n",
       "      <th>total_acc_z_energy</th>\n",
       "      <th>total_acc_z_entropy</th>\n",
       "      <th>total_acc_z_mean_power</th>\n",
       "      <th>total_acc_z_max_freq_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.210534</td>\n",
       "      <td>-0.883335</td>\n",
       "      <td>0.918871</td>\n",
       "      <td>-0.868773</td>\n",
       "      <td>0.611526</td>\n",
       "      <td>0.356362</td>\n",
       "      <td>-0.339400</td>\n",
       "      <td>-0.706303</td>\n",
       "      <td>0.782367</td>\n",
       "      <td>-0.884585</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.913413</td>\n",
       "      <td>0.404670</td>\n",
       "      <td>-0.444325</td>\n",
       "      <td>0.039661</td>\n",
       "      <td>0.299175</td>\n",
       "      <td>0.052158</td>\n",
       "      <td>-0.563109</td>\n",
       "      <td>-0.730411</td>\n",
       "      <td>-1.203022</td>\n",
       "      <td>-0.287719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.060208</td>\n",
       "      <td>-0.890098</td>\n",
       "      <td>0.908664</td>\n",
       "      <td>-0.884263</td>\n",
       "      <td>0.567732</td>\n",
       "      <td>-1.179130</td>\n",
       "      <td>0.580954</td>\n",
       "      <td>-0.706492</td>\n",
       "      <td>1.339885</td>\n",
       "      <td>-0.893425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.900186</td>\n",
       "      <td>0.388502</td>\n",
       "      <td>-0.456851</td>\n",
       "      <td>0.033242</td>\n",
       "      <td>-1.522574</td>\n",
       "      <td>0.556553</td>\n",
       "      <td>-0.565740</td>\n",
       "      <td>-0.593033</td>\n",
       "      <td>-1.195720</td>\n",
       "      <td>-0.287719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows √ó 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_acc_x_mean  body_acc_x_std  body_acc_x_min  body_acc_x_max  \\\n",
       "0         0.210534       -0.883335        0.918871       -0.868773   \n",
       "1         0.060208       -0.890098        0.908664       -0.884263   \n",
       "\n",
       "   body_acc_x_median  body_acc_x_skew  body_acc_x_kurtosis  body_acc_x_energy  \\\n",
       "0           0.611526         0.356362            -0.339400          -0.706303   \n",
       "1           0.567732        -1.179130             0.580954          -0.706492   \n",
       "\n",
       "   body_acc_x_entropy  body_acc_x_mean_power  ...  total_acc_z_std  \\\n",
       "0            0.782367              -0.884585  ...        -0.913413   \n",
       "1            1.339885              -0.893425  ...        -0.900186   \n",
       "\n",
       "   total_acc_z_min  total_acc_z_max  total_acc_z_median  total_acc_z_skew  \\\n",
       "0         0.404670        -0.444325            0.039661          0.299175   \n",
       "1         0.388502        -0.456851            0.033242         -1.522574   \n",
       "\n",
       "   total_acc_z_kurtosis  total_acc_z_energy  total_acc_z_entropy  \\\n",
       "0              0.052158           -0.563109            -0.730411   \n",
       "1              0.556553           -0.565740            -0.593033   \n",
       "\n",
       "   total_acc_z_mean_power  total_acc_z_max_freq_idx  \n",
       "0               -1.203022                 -0.287719  \n",
       "1               -1.195720                 -0.287719  \n",
       "\n",
       "[2 rows x 99 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52f63b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_acc_x_mean</th>\n",
       "      <th>body_acc_x_std</th>\n",
       "      <th>body_acc_x_min</th>\n",
       "      <th>body_acc_x_max</th>\n",
       "      <th>body_acc_x_median</th>\n",
       "      <th>body_acc_x_skew</th>\n",
       "      <th>body_acc_x_kurtosis</th>\n",
       "      <th>body_acc_x_energy</th>\n",
       "      <th>body_acc_x_entropy</th>\n",
       "      <th>body_acc_x_mean_power</th>\n",
       "      <th>...</th>\n",
       "      <th>total_acc_z_std</th>\n",
       "      <th>total_acc_z_min</th>\n",
       "      <th>total_acc_z_max</th>\n",
       "      <th>total_acc_z_median</th>\n",
       "      <th>total_acc_z_skew</th>\n",
       "      <th>total_acc_z_kurtosis</th>\n",
       "      <th>total_acc_z_energy</th>\n",
       "      <th>total_acc_z_entropy</th>\n",
       "      <th>total_acc_z_mean_power</th>\n",
       "      <th>total_acc_z_max_freq_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.210534</td>\n",
       "      <td>-0.883335</td>\n",
       "      <td>0.918871</td>\n",
       "      <td>-0.868773</td>\n",
       "      <td>0.611526</td>\n",
       "      <td>0.356362</td>\n",
       "      <td>-0.339400</td>\n",
       "      <td>-0.706303</td>\n",
       "      <td>0.782367</td>\n",
       "      <td>-0.884585</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.913413</td>\n",
       "      <td>0.404670</td>\n",
       "      <td>-0.444325</td>\n",
       "      <td>0.039661</td>\n",
       "      <td>0.299175</td>\n",
       "      <td>0.052158</td>\n",
       "      <td>-0.563109</td>\n",
       "      <td>-0.730411</td>\n",
       "      <td>-1.203022</td>\n",
       "      <td>-0.287719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.060208</td>\n",
       "      <td>-0.890098</td>\n",
       "      <td>0.908664</td>\n",
       "      <td>-0.884263</td>\n",
       "      <td>0.567732</td>\n",
       "      <td>-1.179130</td>\n",
       "      <td>0.580954</td>\n",
       "      <td>-0.706492</td>\n",
       "      <td>1.339885</td>\n",
       "      <td>-0.893425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.900186</td>\n",
       "      <td>0.388502</td>\n",
       "      <td>-0.456851</td>\n",
       "      <td>0.033242</td>\n",
       "      <td>-1.522574</td>\n",
       "      <td>0.556553</td>\n",
       "      <td>-0.565740</td>\n",
       "      <td>-0.593033</td>\n",
       "      <td>-1.195720</td>\n",
       "      <td>-0.287719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows √ó 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_acc_x_mean  body_acc_x_std  body_acc_x_min  body_acc_x_max  \\\n",
       "0         0.210534       -0.883335        0.918871       -0.868773   \n",
       "1         0.060208       -0.890098        0.908664       -0.884263   \n",
       "\n",
       "   body_acc_x_median  body_acc_x_skew  body_acc_x_kurtosis  body_acc_x_energy  \\\n",
       "0           0.611526         0.356362            -0.339400          -0.706303   \n",
       "1           0.567732        -1.179130             0.580954          -0.706492   \n",
       "\n",
       "   body_acc_x_entropy  body_acc_x_mean_power  ...  total_acc_z_std  \\\n",
       "0            0.782367              -0.884585  ...        -0.913413   \n",
       "1            1.339885              -0.893425  ...        -0.900186   \n",
       "\n",
       "   total_acc_z_min  total_acc_z_max  total_acc_z_median  total_acc_z_skew  \\\n",
       "0         0.404670        -0.444325            0.039661          0.299175   \n",
       "1         0.388502        -0.456851            0.033242         -1.522574   \n",
       "\n",
       "   total_acc_z_kurtosis  total_acc_z_energy  total_acc_z_entropy  \\\n",
       "0              0.052158           -0.563109            -0.730411   \n",
       "1              0.556553           -0.565740            -0.593033   \n",
       "\n",
       "   total_acc_z_mean_power  total_acc_z_max_freq_idx  \n",
       "0               -1.203022                 -0.287719  \n",
       "1               -1.195720                 -0.287719  \n",
       "\n",
       "[2 rows x 99 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_CLEAN = X_scaled_df\n",
    "X_CLEAN.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230c0a78",
   "metadata": {},
   "source": [
    "# SAVING THE FILES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbfa26ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All files saved under: /Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/DATA_EXTRACTED_WITH_99_FEATURES\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define base path\n",
    "base_path = \"/Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/DATA_EXTRACTED_WITH_99_FEATURES\"\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "# Save cleaned feature data\n",
    "X_CLEAN.to_csv(os.path.join(base_path, \"X_CLEAN.csv\"), index=False)\n",
    "\n",
    "# Save labels\n",
    "y_df = pd.DataFrame(y, columns=['activity'])\n",
    "y_df.to_csv(os.path.join(base_path, \"Y_CLEAN.csv\"), index=False)\n",
    "\n",
    "# Save feature names\n",
    "feature_names = df_features.columns.tolist()\n",
    "with open(os.path.join(base_path, \"feature_names.txt\"), 'w') as f:\n",
    "    for name in feature_names:\n",
    "        f.write(f\"{name}\\n\")\n",
    "\n",
    "# Also save feature names in CSV\n",
    "feature_names_df = pd.DataFrame(feature_names, columns=['feature_name'])\n",
    "feature_names_df.to_csv(os.path.join(base_path, \"feature_names.csv\"), index=False)\n",
    "\n",
    "# Save shape info\n",
    "shape_info = {\n",
    "    'features_shape': df_features.shape,\n",
    "    'labels_shape': y_df.shape\n",
    "}\n",
    "shape_info_df = pd.DataFrame([shape_info])\n",
    "shape_info_df.to_csv(os.path.join(base_path, \"shape_info.csv\"), index=False)\n",
    "\n",
    "print(\" All files saved under:\", base_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6697251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README file created at: /Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE/README.txt\n"
     ]
    }
   ],
   "source": [
    "readme_text = \"\"\"\n",
    "FOLDER STRUCTURE AND CONTENTS\n",
    "============================================================\n",
    "\n",
    "/stage1_BASELINE/ \n",
    "‚îÇ__DATA_EXTRACTED_WITH_99_FEATURES/ # Contains cleaned and preprocessed data\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ feature_names.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ feature_names.txt\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ shape_info.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ X_CLEAN.csv\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Y_CLEAN.csv\n",
    "|__OUTPUT/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ <MODEL_NAME>_RESULTS/  # e.g., LinearSVC_RESULTS/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ classification_report.txt\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ confusion_matrix.png\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ model_comparison.txt\n",
    "|__README.txt\n",
    "------------------------------------------------------------\n",
    "\n",
    "      \n",
    "      \n",
    "\n",
    "Folder: DATA_EXTRACTED_WITH_99_FEATURES\n",
    "------------------------------------------------------------\n",
    "This folder contains the cleaned and preprocessed data extracted from the **UCI HAR Dataset** using raw inertial signal files.\n",
    "\n",
    "‚úîÔ∏è SOURCE:\n",
    "-----------\n",
    "The raw data was taken from the Inertial Signals directory of the original UCI HAR dataset. These signals include 9 sensor signals:\n",
    "- body_acc_x\n",
    "- body_acc_y\n",
    "- body_acc_z\n",
    "- body_gyro_x\n",
    "- body_gyro_y\n",
    "- body_gyro_z\n",
    "- total_acc_x\n",
    "- total_acc_y\n",
    "- total_acc_z\n",
    "\n",
    "Each signal was recorded over 128 time steps for every activity window/sample.\n",
    "\n",
    "‚úîÔ∏è PIPELINE:\n",
    "------------\n",
    "1. **Raw Data Loaded**  \n",
    "   Inertial signal files from `train` and `test` directories were loaded and concatenated to form the full raw dataset.\n",
    "\n",
    "2. **Raw Files Saved**  \n",
    "   Raw signal data was saved as `X_RAW.csv` and labels as `Y_RAW.csv` for reference.\n",
    "\n",
    "3. **Feature Extraction**  \n",
    "   From each window (i.e., one sample of 128 time steps), the following statistical and frequency-domain features were extracted:\n",
    "   \n",
    "   For each signal (total 9), the following 11 features were computed:\n",
    "   - Mean\n",
    "   - Standard Deviation\n",
    "   - Minimum\n",
    "   - Maximum\n",
    "   - Median\n",
    "   - Skewness\n",
    "   - Kurtosis\n",
    "   - Energy (Sum of squares of FFT)\n",
    "   - Entropy (of normalized FFT)\n",
    "   - Mean Power (mean of FFT magnitudes)\n",
    "   - Max Frequency Index (argmax of FFT)\n",
    "\n",
    "   üëâ This results in **99 features total** (9 signals √ó 11 features each).\n",
    "\n",
    "4. **Cleaned Data Saved**  \n",
    "   The extracted features were saved as:\n",
    "   - `X_CLEAN.csv` ‚Äî Cleaned feature matrix (shape: [samples, 99])\n",
    "   - `Y_RAW.csv` ‚Äî Corresponding labels for each sample\n",
    "   - `feature_names.csv` ‚Äî Names of the 99 features\n",
    "   - `feature_names.txt` ‚Äî Plain text list of all features\n",
    "   - `shape_info.csv` ‚Äî Shape of the final feature and label datasets\n",
    "\n",
    "‚úîÔ∏è OUTPUT FILES:\n",
    "----------------\n",
    "- `X_CLEAN.csv` ‚Äî Extracted feature dataset\n",
    "- `Y_RAW.csv` ‚Äî Activity labels\n",
    "- `feature_names.csv` ‚Äî Feature names in CSV\n",
    "- `feature_names.txt` ‚Äî Feature names in plain text\n",
    "- `shape_info.csv` ‚Äî Dataset shapes\n",
    "\n",
    "üìå This dataset is now ready for use in machine learning pipelines for Human Activity Recognition (HAR).\n",
    "\n",
    "Author: Priyam Pandey  \n",
    "Date: [24 TH MAY 2025]\n",
    "\n",
    "---------------------\n",
    "MODEL TRAINING AND EVALUATION\n",
    "------------------------------------------------------------\n",
    "* Loaded `X_CLEAN.csv` and `Y_CLEAN.csv` from the given base path\n",
    "\n",
    "* Split data into training and testing sets\n",
    "\n",
    "* Defined 12 classification models:\n",
    "  `[LinearSVC, GradientBoosting, ExtraTrees, Bagging, ANN, RandomForest, CART, GaussianNB, DecisionTree, AdaBoost, KNN, LogisticRegression]`\n",
    "\n",
    "* Trained each model using a loop\n",
    "\n",
    "* Calculated metrics: Accuracy, F1 Score, Recall, Precision\n",
    "* Saved classification report (`.txt`) and confusion matrix (`.png`) for each model in a separate folder named `<MODEL_NAME>_RESULTS`\n",
    "* Compiled all model scores into a comparison table, saved as `model_comparison.txt` in the base path\n",
    "------------------------------------------------------------\n",
    "Folder Structure:\n",
    "\n",
    "\n",
    "/stage1_BASELINE/\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ X_CLEAN.csv\n",
    "‚îú‚îÄ‚îÄ Y_CLEAN.csv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "base_path = \"/Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE\"\n",
    "with open(os.path.join(base_path, \"README.txt\"), \"w\") as f:\n",
    "    f.write(readme_text)\n",
    "print(\"README file created at:\", os.path.join(base_path, \"README.txt\"))\n",
    "# Save the README file in the base path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38922a35",
   "metadata": {},
   "source": [
    "# MODEL_TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3bb53d",
   "metadata": {},
   "source": [
    "IMPORT LIBRAIES \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "528ff3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, recall_score, precision_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "# ML models\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier,\n",
    "    RandomForestClassifier, AdaBoostClassifier\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c7af1f",
   "metadata": {},
   "source": [
    "### LOADING DATA AND SPLITTING IT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20323e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base path\n",
    "base_path1 = \"/Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE/DATA_EXTRACTED_WITH_99_FEATURES\"\n",
    "\n",
    "# Load clean data\n",
    "X = pd.read_csv(os.path.join(base_path1, \"X_CLEAN.csv\"))\n",
    "y = pd.read_csv(os.path.join(base_path1, \"Y_CLEAN.csv\")).values.ravel()\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36169851",
   "metadata": {},
   "source": [
    "### DEFINING MODEL DIRECTORY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a1e8fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path= \"/Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE/OUTPUT1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4c7e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of models\n",
    "models = {\n",
    "    \"Linear_SVC\": LinearSVC(max_iter=10000),\n",
    "    \"Gradient_Boosting\": GradientBoostingClassifier(),\n",
    "    \"Extra_Trees\": ExtraTreesClassifier(),\n",
    "    \"Bagged_Decision_Trees\": BaggingClassifier(),\n",
    "    \"ANN\": MLPClassifier(max_iter=1000),\n",
    "    \"Random_Forest\": RandomForestClassifier(),\n",
    "    \"CART\": DecisionTreeClassifier(),  # Same as Decision Tree\n",
    "    \"Gaussian_Naive_Bayes\": GaussianNB(),\n",
    "    \"Decision_Tree\": DecisionTreeClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Logistic_Regression\": LogisticRegression(max_iter=10000)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2f8bf",
   "metadata": {},
   "source": [
    "### üîÅ Train, Evaluate, Save Report & Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f29b56db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Linear_SVC...\n",
      "Linear_SVC completed. Results saved in /Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE/OUTPUT1/Linear_SVC_RESULTS\n",
      "Training Gradient_Boosting...\n",
      "Gradient_Boosting completed. Results saved in /Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE/OUTPUT1/Gradient_Boosting_RESULTS\n",
      "Training Extra_Trees...\n",
      "Extra_Trees completed. Results saved in /Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE/OUTPUT1/Extra_Trees_RESULTS\n",
      "Training Bagged_Decision_Trees...\n",
      "Bagged_Decision_Trees completed. Results saved in /Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE/OUTPUT1/Bagged_Decision_Trees_RESULTS\n",
      "Training ANN...\n",
      "ANN completed. Results saved in /Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE/OUTPUT1/ANN_RESULTS\n",
      "Training Random_Forest...\n",
      "Random_Forest completed. Results saved in /Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE/OUTPUT1/Random_Forest_RESULTS\n",
      "Training CART...\n",
      "CART completed. Results saved in /Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE/OUTPUT1/CART_RESULTS\n",
      "Training Gaussian_Naive_Bayes...\n",
      "Gaussian_Naive_Bayes completed. Results saved in /Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE/OUTPUT1/Gaussian_Naive_Bayes_RESULTS\n",
      "Training Decision_Tree...\n",
      "Decision_Tree completed. Results saved in /Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE/OUTPUT1/Decision_Tree_RESULTS\n",
      "Training AdaBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost completed. Results saved in /Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE/OUTPUT1/AdaBoost_RESULTS\n",
      "Training KNN...\n",
      "KNN completed. Results saved in /Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE/OUTPUT1/KNN_RESULTS\n",
      "Training Logistic_Regression...\n",
      "Logistic_Regression completed. Results saved in /Users/priyam/paper_recreation/HAR MODEL_OPTIMIZATION/stage1_BASELINE/OUTPUT1/Logistic_Regression_RESULTS\n"
     ]
    }
   ],
   "source": [
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Loop through each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    results.append([model_name, acc, f1, recall, precision])\n",
    "\n",
    "    # Classification report & confusion matrix\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Save in model-specific folder\n",
    "    model_folder = os.path.join(base_path, f\"{model_name}_RESULTS\")\n",
    "    os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(model_folder, f\"{model_name}_classification_report.txt\"), \"w\") as f:\n",
    "        f.write(report)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.savefig(os.path.join(model_folder, f\"{model_name}_confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "    print(f\"{model_name} completed. Results saved in {model_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b6c8c8",
   "metadata": {},
   "source": [
    "### SAVE COMPARISON TABLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "524164fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All models trained and results saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create and sort comparison DataFrame\n",
    "comparison_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"F1 Score\", \"Recall\", \"Precision\"])\n",
    "comparison_df.sort_values(by=\"F1 Score\", ascending=False, inplace=True)\n",
    "\n",
    "# Save to TXT\n",
    "comparison_txt_path = os.path.join(base_path, \"model_comparison.txt\")\n",
    "with open(comparison_txt_path, \"w\") as f:\n",
    "    f.write(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"‚úÖ All models trained and results saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdda58c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
